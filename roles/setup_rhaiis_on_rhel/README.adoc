= Red Hat AI Inference Server (RHAIIS) Setup Role

This role configures a RHEL system to run Red Hat AI Inference Server using vLLM in a containerized environment with NVIDIA GPU support.

== Features

* NVIDIA GPU support with CDI configuration
* Container registry authentication for Red Hat registries
* HuggingFace model pre-download for improved reliability
* Systemd service management for RHAIIS
* Health checks and monitoring capabilities

== Requirements

* RHEL-based system (RHEL 8.6+, RHEL 9+)
* NVIDIA GPU with compatible drivers
* Podman container runtime
* Internet connectivity for downloading models and container images
* Valid HuggingFace token for model access
* Valid Red Hat registry credentials

== Role Variables

=== Authentication
[source,yaml]
----
setup_rhaiis_on_rhel_hf_token: secret                    # HuggingFace API token
setup_rhaiis_on_rhel_api_token: secret                   # RHAIIS API token for authentication
----

=== User Configuration
[source,yaml]
----
setup_rhaiis_on_rhel_target_user: "{{ ansible_user | default(ansible_user_id | default('ec2-user')) }}"
----

=== vLLM Configuration
[source,yaml]
----
setup_rhaiis_on_rhel_vllm_cache_dir: "/home/{{ setup_rhaiis_on_rhel_target_user }}/.cache"
setup_rhaiis_on_rhel_vllm_port: 8000
setup_rhaiis_on_rhel_vllm_tensor_parallel_size: 1
setup_rhaiis_on_rhel_vllm_max_model_len: 2048
setup_rhaiis_on_rhel_vllm_model: "RedHatAI/Mistral-7B-Instruct-v0.3-FP8"
setup_rhaiis_on_rhel_vllm_container_name: "rhaiis"
setup_rhaiis_on_rhel_vllm_container_image: "registry.redhat.io/rhaiis/vllm-cuda-rhel9"
----

=== Model Pre-download
[source,yaml]
----
setup_rhaiis_on_rhel_predownload_model: true             # Enable model pre-download (recommended)
setup_rhaiis_on_rhel_download_timeout: 600               # Download timeout in seconds (10 minutes)
setup_rhaiis_on_rhel_download_retries: 3                 # Number of download retry attempts
setup_rhaiis_on_rhel_download_retry_delay: 60            # Delay between retries in seconds
setup_rhaiis_on_rhel_download_async_timeout: 1200        # Ansible async timeout (20 minutes)
setup_rhaiis_on_rhel_download_poll_interval: 30          # Ansible polling interval in seconds
----

=== Registry Authentication
[source,yaml]
----
setup_rhaiis_on_rhel_registry_auth_enabled: true
setup_rhaiis_on_rhel_registry_url: "registry.redhat.io"
setup_rhaiis_on_rhel_registry_username: ""
setup_rhaiis_on_rhel_registry_password: ""
setup_rhaiis_on_rhel_registry_auth_token: ""             # Alternative to username/password
----

=== Service Health Checks
[source,yaml]
----
setup_rhaiis_on_rhel_health_check_retries: 30            # Total retries (30 x 30s = 15 minutes)
setup_rhaiis_on_rhel_health_check_delay: 30              # Seconds between retries
setup_rhaiis_on_rhel_health_endpoint: "/docs"            # Health check endpoint
----

== Model Pre-download Feature

This role includes intelligent model pre-download functionality that:

* Downloads models to the local cache before starting the service
* Improves service startup reliability and speed
* Supports resume capability for interrupted downloads
* Verifies successful download before proceeding
* Can be disabled by setting `setup_rhaiis_on_rhel_predownload_model: false`

The pre-download process:

1. Installs HuggingFace CLI tools
2. Checks if the model is already cached
3. Downloads the model with retry logic (3 attempts)
4. Verifies successful download
5. Maps the cache directory into the container

== Example Playbook

[source,yaml]
----
- hosts: rhel_servers
  become: yes
  vars:
    setup_rhaiis_on_rhel_hf_token: "{{ vault_hf_token }}"
    setup_rhaiis_on_rhel_api_token: "{{ vault_api_token }}"
    setup_rhaiis_on_rhel_registry_username: "{{ vault_registry_username }}"
    setup_rhaiis_on_rhel_registry_password: "{{ vault_registry_password }}"
    setup_rhaiis_on_rhel_vllm_model: "RedHatAI/Mistral-7B-Instruct-v0.3-FP8"
  roles:
    - setup_rhaiis_on_rhel
----

== Dependencies

This role depends on the `setup_nvidia_cuda` role for GPU driver installation and configuration.

== Troubleshooting

=== Model Download Issues
If model downloads fail or get stuck:

1. Check HuggingFace token validity
2. Verify internet connectivity
3. Check available disk space in cache directory
4. Adjust timeout settings if needed:
   - Increase `setup_rhaiis_on_rhel_download_timeout` for slower connections
   - Increase `setup_rhaiis_on_rhel_download_retries` for unreliable networks
   - Adjust `setup_rhaiis_on_rhel_download_retry_delay` between attempts
5. Manually test: `huggingface-cli download MODEL_NAME --token YOUR_TOKEN`

=== Service Startup Issues
Check service logs:
[source,bash]
----
sudo journalctl -u rhaiis -f
cat /tmp/rhaiis.log
----

=== Container Registry Issues
Verify registry authentication:
[source,bash]
----
podman login registry.redhat.io
podman pull registry.redhat.io/rhaiis/vllm-cuda-rhel9
----

== License

Apache License 2.0

== Author Information

This role was created for deploying Red Hat AI Inference Server on RHEL systems with GPU acceleration support.
